## create seeds, only has to be done once (depending on the number of runs per simulation)

# yanis todo:
# todo: fix folder system for version that will be made available to public/published so it agrees with:
# https://snakemake.readthedocs.io/en/stable/snakefiles/deployment.html
# simngs, wgsim
# yaahs - juicer \ module
# blocktools / conda
# how to simulate diploid genome? alt assembly or mutation simulator?
# add megahit
# assemblers { 
# pb-falcon / pb-assembly
# peregrine-2021
# trio-binning
# dipasm
# hi-canu / conda as canu }
# ipa \ conda pbipa 
# benchamrking for hifi assembler: 
# https://www.biorxiv.org/content/10.1101/2022.02.15.480579v1.full



######### Changes #########
# found more pythonic way for seed list
# added a sample prefix, so user can specify the sample
# changed output looper to include sample name
# wildcard constraints so snakemake doesnt get confused
# small changes to hifiasm
# changed awk command
# removed module load, all programs should be in conda environment for reproducibility
# adapted script to sg cluster with module and file location changes
# add script for combining scaffolds to make simulations easier
# add contamination (here from ecoli and human equalling 1%)
# add resouce_mem restrictions
# save larger files to specific folder "share/pool" to be on the safe side

import random
import os

# CONFIG
# for i in range(0,10):
#     n = random.randint(1000,100000)
#     seed_list.append(n)
# seed_list = ["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20"]

configfile: "config.yaml"

seed_list=[format(x, '02d') for x in range(1,21,1)]

## create chr_nm list (0001 - 0067)
# find out the number of scaffolds of your assembly with : 
# samtools view path/to/fastafile | wc -l
# and verify by making sure it matches the ncbi entry
# keep all scaffolds because eventhough they may be unplaced they're still part of the genome
CHROMS = ["%04d" % x for x in range(1,config["chrom_num"]+1)]

## create a haplotype list
hap_list = ["hap1", "hap2"]

#name of sample 
sample_prefix=config["sample_prefix"]

# path to seed hap 1 & 2
# and to genomes to be used for contamination sims

SEED_HAP_1=config["genomes"]["SEED_HAP_1"]
SEED_HAP_2=config["genomes"]["SEED_HAP_2"]


#busco db to use
LINEAGE=config["LINEAGE"]

#######################################

# restrict wildcards to avoid smk confusion
wildcard_constraints:
    hap="hap[0-9]",
    seeds="[0-9]+",
    chr_nm="[0-9]{4}",
    cont="cont[0-9]",
    chrn1="[0-9]{4}",
    chrn2="[0-9]{4}",
    cont_file="cont[0-9]_[0-9]{4}"

#######################################

# set up wildcards for contaminant DNA:
# given we simulate 60X coverage, a 1% contamination (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3487130/)
# would require total 3X contaminants in total for a 30X 
# let's decide the amount of depth at random:

# NOTE: pbsim depth can't be 0
# calculate contamination depth based on the genome depth
# so that the extra contamination will be 5% of the total amount of DNA:

# relative_cont_depth = depth/(95%) - depth
# here depth is relative to 1 simulation, se we multiply it by the number of simulations done
# assuming that if there is contamination the amount of bacterial cont. will be higher than the human cont. 
# so we want a scenario where more than 50% of contamination comes from bacteria (cont 1)

rcd=config["depth"]*20*2/0.95 - config["depth"]*20*2

# percentage of bacterial and human contamination
# as a proportion of the total contamination
cont1_percent=round(random.uniform(0.5, 1),3)
cont2_percent=1-cont1_percent

# depth number to be used by pbsim
cont1depth=cont1_percent*rcd
cont2depth=cont2_percent*rcd
# to find genomes:
# esearch -db nuccore -query "GCF_003018455.1" | efetch -format fasta > DATA/ecoli.fasta
#  wget https://s3.amazonaws.com/genomeark/species/Homo_sapiens/mHomSap3/assembly_curated/mHomSap3.mat.cur.20200716.fasta.gz DATA/human/

contaminant_1=config["genomes"]["contam1"]
contaminant_2=config["genomes"]["contam2"]

# count number of chromosomes for each file
chromnum_cont1 = len([1 for line in open(contaminant_1) if line.startswith(">")])
chromnum_cont1_list = ["%04d" % x for x in range(1,chromnum_cont1+1)]
chromnum_cont2 = len([1 for line in open(contaminant_2) if line.startswith(">")])
chromnum_cont2_list = ["%04d" % x for x in range(1,chromnum_cont2+1)]

cont_prefix=expand("cont1_{chrn1}",chrn1=chromnum_cont1_list) + expand("cont2_{chrn2}",chrn2=chromnum_cont2_list)

#######################################

# example:
# 01_SAMPLE_hap1/01_SAMPLE_0025
# "['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20']\
# _lynCan1_hap1/['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20']\
# _lynCan1_0028.sam"
#for each sample, for each haplotype, simulate each chromosome a seed number of times
#each chromosome will be simulated a seed # of times
def output_looper(seed_name, pref, hap, cat=""):
    for i in CHROMS:
        if cat == "maf":
            yield "/share/pool/simulations_collomics/{0}_{1}_{2}/{0}_{1}_{3}.maf".format(seed_name, pref, hap, i)
        elif cat == "ref":
            yield "/share/pool/simulations_collomics/{0}_{1}_{2}/{0}_{1}_{3}.ref".format(seed_name, pref, hap, i)
        elif cat == "sam":
            yield "/share/pool/simulations_collomics/{0}_{1}_{2}/{0}_{1}_{3}.sam".format(seed_name, pref, hap, i)

## desired output after one full run

#######################################

rule all:
    input:
        expand("bam_files/{seeds}_{hap}_{sample}_{chr_nm}.bam", seeds=seed_list, hap=hap_list, chr_nm=CHROMS,sample=sample_prefix),
        expand("busco/{sample}/run_{db}/missing_busco_list.tsv",db=LINEAGE,sample=sample_prefix),
        expand("bam_files/cont1_{chrn1}.bam",chrn1=chromnum_cont1_list),
        expand("bam_files/cont2_{chrn2}.bam",chrn2=chromnum_cont2_list),
        expand("fastq_files/{cont_file}.fastq.gz",cont_file=cont_prefix),
        "stats/backmapping_stats.txt",
        "stats/assembly_stats.txt",
        "stats/read_stats.txt"

#######################################

## Step 1: Simulate and reassemble reads for both haplotypes 
## simulate reads with pbsim, for different read lengths, one can adjust the length per read in
## the shell command. If coverage per run is 3, 10 directories with output files can be generated in about 3 hours
#output_looper(seed_name={seed_list}, hap=hap_list[0], cat="sam")
rule run_sim_hap1:
    input: 
        genome = SEED_HAP_1
    output:
        bam = temp(output_looper("{seeds}", "{sample}", hap=hap_list[0], cat="sam")),
        ref = temp(output_looper("{seeds}", "{sample}", hap=hap_list[0], cat="ref")),
        maf = temp(output_looper("{seeds}", "{sample}", hap=hap_list[0], cat="maf"))
    params:
       	seed_id = "{seeds}",
        prefix_sim = "/share/pool/simulations_collomics/{seeds}_{sample}_hap1/{seeds}_{sample}",
        lngth = config["read_length"],
        dpth = config["depth"]
#    envmodules:
#        "pbsim3/3.0.0"
    resources:
        mem_mb=100000
    shell: """
        module load pbsim3/3.0.0
        pbsim --prefix {params.prefix_sim} --strategy wgs --genome {input.genome} \
        --depth {params.dpth} --method errhmm --errhmm /share/scientific_src/pbsim3/3.0.0/data/ERRHMM-SEQUEL.model \
        --length-mean {params.lngth} --pass-num 10 --seed {params.seed_id} 
    """

rule run_sim_hap2:
    input: 
        genome = SEED_HAP_2 ## fasta file was artificially mutated with SNPs and Indels
    output:
        bam = temp(output_looper("{seeds}", "{sample}", hap=hap_list[1], cat="sam")),
        ref = temp(output_looper("{seeds}", "{sample}", hap=hap_list[1], cat="ref")),
        maf = temp(output_looper("{seeds}", "{sample}", hap=hap_list[1], cat="maf"))
    params:
       	seed_id = "{seeds}",
        prefix_sim = "/share/pool/simulations_collomics/{seeds}_{sample}_hap2/{seeds}_{sample}",
        lngth = config["read_length"],
        dpth = config["depth"]
#    envmodules:
#        "pbsim3/3.0.0"
    resources:
        mem_mb=100000
    shell: """
        module load pbsim3/3.0.0
        pbsim --prefix {params.prefix_sim} --strategy wgs --genome {input.genome} \
        --depth {params.dpth} --method errhmm --errhmm /share/scientific_src/pbsim3/3.0.0/data/ERRHMM-SEQUEL.model \
        --length-mean {params.lngth} --pass-num 10 --seed {params.seed_id} 
    """

## convert sam output from simulations to bam

rule make_bam:
    input:
        sam = "/share/pool/simulations_collomics/{seeds}_{sample}_{hap}/{seeds}_{sample}_{chr_nm}.sam"
    output:
        bam = "bam_files/{seeds}_{hap}_{sample}_{chr_nm}.bam"
    shell:
        "samtools view -b {input.sam} -o {output.bam} --threads 30"

## get HiFi reads

# if we want deep_consensus replace this rule with the code inside deepconsensus.smk
rule run_ccs:
    input:
        bam = "bam_files/{seeds}_{hap}_{sample}_{chr_nm}.bam"
    output:
        fastq = "fastq_files/{seeds}_{hap}_{sample}_{chr_nm}.fastq.gz"
    resources:
        mem_mb=100000
    conda:
        "envs/ccs.yml"
    shell:
        "ccs {input.bam} {output.fastq} -j 5"

#######################################

# do simulations for contaminants
include: "rules/contamination_sims.smk"

#######################################


## concatenate reads 

rule cat_fastq:
    input:
        fastq = expand("fastq_files/{seeds}_{hap}_{sample}_{chr_nm}.fastq.gz", seeds=seed_list, hap=hap_list, chr_nm=CHROMS,sample=sample_prefix),
        contam = expand("fastq_files/{cont_file}.fastq.gz",cont_file=cont_prefix)
    output:
        "fastq_files/all_pbsim.fastq.gz"
    shell:
        "cat {input.fastq} {input.contam} > {output}"

## assemble with hifiasm using default parameters

#######################################

rule hifiasm_assembly:
    input:
        "fastq_files/all_pbsim.fastq.gz"
    output:
        "assembly/pbsim_asm.p_ctg.gfa"
    threads:
        workflow.cores
    params:
        inputpre="assembly/pbsim_asm"
    conda:
        "envs/hifiasm.yml"
    shell:
        "hifiasm --primary -l2 -t {threads} -o {params.inputpre} {input}"

#######################################

## Step 2: Backmapping and stats

## convert gfa to fasta

rule gfa_to_fa:
    input:
        "assembly/pbsim_asm.p_ctg.gfa"
    output:
        "assembly/pbsim_asm.p_ctg.fa"
    shell: """
        awk '/^S/{{print ">"$2;print $3}}' {input} > {output}
    """

## compute basic summary stats for the assembly

rule assembly_stats:
    input:
        "assembly/pbsim_asm.p_ctg.fa"
    output:
        "stats/assembly_stats.txt"
    shell:
        "seqkit stats -a -T {input} > {output}"

#######################################

# BUSCO Score

# -m: genome mode
# -i: input
# -o: output folder
# -l: lineage/database - define it in config file
# --out-path: general folder to place output
# download_path: where to find the lineage/db information
#vertebrata_odb9 and
#tetrapoda_odb9
rule RUN_BUSCO:
    input:
        "assembly/pbsim_asm.p_ctg.fa"
    output:
        "busco/{sample}/run_{db}/missing_busco_list.tsv"
    threads:
        workflow.cores
    params:
        dataset_dir="busco_downloads",
        out_dir="busco/",
        run_name=sample_prefix,
        lineage=LINEAGE
    conda:
        "envs/busco.yml"
    shell: """
        busco -m genome \
        -c {threads} \
        -i {input} \
        -o {params.run_name}
        --download_path {params.dataset_dir} \
        --out_path {params.out_dir} \
        -l {params.lineage} \
        --offline
    """
#######################################

## shred assembly into 150 bp long "reads" with 50 bp overlap, creating faux reads with overall 1.5 x coverage
# in a conda environment you should be able to call shred.sh from the terminal like normal
rule backmapping_shredding:
    input:
        "assembly/pbsim_asm.p_ctg.fa"
    output:
        "split_assembly/split_assembly.fa"
    conda:
        "envs/bbtools.yml"
    shell:
        "shred.sh -Xmx10g in={input} out={output} length=150 minlength=10 overlap=50"

## map these reads back to the original input file

rule backmapping_mapping:
    input:
        asm = "split_assembly/split_assembly.fa",
        genome = SEED_HAP_1
    output:
        temp("split_assembly/backmapped.sam")
    threads:
        config["CORES"]
    conda:
        "envs/bwa.yml"
    shell:
        "bwa mem {input.genome} {input.asm} -t {threads} > {output}"

## convert sam to bam

rule backmapping_bam:
    input:
        "split_assembly/backmapped.sam"
    output:
        temp("split_assembly/backmapped.bam")
    shell:
        "samtools view -b -o {output} {input} --threads 40"

## sort bam file

rule backmapping_sort:
    input:
        "split_assembly/backmapped.bam"
    output:
        temp("split_assembly/backmapped.srt.bam")
    shell:
        "samtools sort {input} --threads 40 > {output}"

## compute average number of bases overlapped by at least one read

rule backmapping_stats:
    input:
        "split_assembly/backmapped.srt.bam"
    output:
        "stats/backmapping_stats.txt"
    shell:
        "samtools coverage -m -A {input} > {output}"



rule read_stats:
    input:
        "fastq_files/all_pbsim.fastq.gz"
    output:
        "stats/read_stats.txt"
    conda:
        "envs/seqkit.yml"
    shell:
        "seqkit stats -a -T {input} > {output}"

